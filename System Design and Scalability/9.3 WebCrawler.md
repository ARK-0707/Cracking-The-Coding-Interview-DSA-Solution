## 9.3 Web Crawler

## Problem Description
If you were designing a web crawler, how would you avoid getting into infinite loops?

## Solution

Designing a web crawler to avoid infinite loops requires careful consideration at both a high level and in implementation details. Here's a **system design perspective** to address this:

---

### **1. System Architecture Overview**
- **Components**: 
  - **URL Frontier**: Manages the queue of URLs to be crawled.
  - **Downloader**: Fetches web pages.
  - **Parser**: Extracts links and relevant content.
  - **Storage**: Maintains crawled data and metadata.
  - **Duplicate Detection Module**: Ensures no URL or page is crawled multiple times.
  
---

### **2. Strategies to Prevent Infinite Loops**
#### **A. URL Normalization**
- **Problem**: Different URLs can represent the same resource (e.g., `example.com`, `www.example.com`, or `example.com/`).
- **Solution**:
  - Normalize URLs by removing unnecessary parameters (like `utm_source`), converting to lowercase, and resolving relative paths.
  - Store normalized URLs in the database or hash table for efficient lookups.

#### **B. Use a Visited Set**
- **Problem**: Re-visiting the same URLs repeatedly causes loops.
- **Solution**:
  - Maintain a **Visited Set** (or database) to track already crawled URLs.
  - Before crawling a URL, check if it exists in the Visited Set.

#### **C. Detect and Avoid Cyclic Links**
- **Problem**: Pages linking in a loop (A → B → C → A).
- **Solution**:
  - Implement a depth-bound or path-tracking mechanism:
    - Limit the crawling depth or the number of hops.
    - Track visited pages in the current path and avoid revisiting nodes.

#### **D. Robust URL Frontier Management**
- **Problem**: Inefficient queue management may reintroduce crawled URLs.
- **Solution**:
  - Use a priority queue or distributed queue system to ensure only unique and relevant URLs are processed.
  - De-duplicate entries in the queue based on the Visited Set.

#### **E. Politeness and Rate Limiting**
- **Problem**: Crawlers may repeatedly hit the same site due to high link density.
- **Solution**:
  - Use a **per-domain crawl delay** to avoid overwhelming servers and re-crawling similar content.
  - This reduces accidental loops caused by high-frequency fetch requests.

#### **F. Content Hashing**
- **Problem**: Some sites generate duplicate content under different URLs.
- **Solution**:
  - Compute a hash of the page's content (like SHA256) and store it in a database.
  - Compare hashes of newly fetched pages with previously stored hashes to detect duplicates.

---

### **3. Algorithm for Crawling**
1. **Initialize**: Start with a seed list of URLs.
2. **Fetch**: Pick a URL from the URL Frontier.
3. **Normalize and Check**: Normalize the URL and check against the Visited Set or content hashes.
   - **If already visited**, skip.
   - **If new**, add to the Visited Set.
4. **Parse**: Extract links and validate them (e.g., remove non-HTTP(s) links, broken URLs).
5. **Push Valid URLs to Frontier**: Add only valid and unvisited URLs to the URL Frontier.
6. **Repeat**: Continue fetching URLs until the crawling depth or rate limit is reached.

---

### **4. Database and Storage Design**
- **Visited Set**:
  - Use a high-performance database or in-memory key-value store (e.g., Redis) to store normalized URLs.
  - Use consistent hashing to distribute the Visited Set in large-scale distributed crawlers.
- **Content Storage**:
  - Store metadata (e.g., URL, fetch time, content hash) and fetched pages in a scalable storage solution (e.g., Amazon S3, Hadoop).

---

### **5. Additional Considerations**
#### **A. Avoid Traps Like Calendars**
- Many websites dynamically generate infinite links (e.g., a calendar where you can infinitely go to "next month").
- Implement heuristic rules to detect and limit such patterns (e.g., look for URLs with repetitive patterns).

#### **B. Respect Robots.txt**
- Crawlers should obey the `robots.txt` protocol to avoid crawling restricted or unnecessary pages.

#### **C. Monitor and Debug**
- Add logging and monitoring for crawler activity.
- Track the frequency of revisits or looping patterns to identify issues.

#### **D. Distributed Design**
- In a distributed crawler, synchronize the Visited Set across nodes using consistent hashing or a distributed database to avoid duplications and inefficiencies.

---

### **6. Scaling the System**
- Use **sharding** to partition the URL Frontier and Visited Set across multiple servers.
- Implement **failover mechanisms** to handle node failures without data loss.
- Optimize by prioritizing URLs (e.g., popular domains, freshness).

This design should ensure the crawler operates efficiently without getting trapped in infinite loops.