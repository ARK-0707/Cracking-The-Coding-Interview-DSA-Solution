# 9.5 Cache

## Problem Description
Imagine a web server for simplified search engine. This system has 100 machines to respond to search queries, which may 
then call out using `processSearch(string query)` to another cluster of machines to actually get the result. The machine
which responds to a given query is chosen at random, so you cannot guarantee that the same machine will always respond
to the same request. The method `processSearch` is very expensive. Design a caching mechanism for the most recent queries.
Be sure to explain how you would update the cache when data changes.

## Solution

Designing a **distributed caching mechanism** for a simplified search engine with 100 machines requires careful consideration of **scalability**, **consistency**, and **efficiency**. Hereâ€™s how we can design it:

---

### **1. Problem Requirements**
1. **Random Assignment**: Any of the 100 machines can respond to a query, so the cache needs to be shared or coordinated across them.
2. **Expensive Search**: The method `processSearch(query)` is costly, so caching results is essential to reduce load and latency.
3. **Dynamic Data Updates**: The cache must reflect data changes in a timely manner to avoid serving stale results.

---

### **2. System Design**
#### **A. Architecture Overview**
1. **Distributed Cache**:
   - Use a **shared cache layer** that all 100 machines can access, such as Redis, Memcached, or a custom distributed hash table (DHT).
2. **Cache Key**:
   - Use the search query string (or a hashed version) as the cache key.
3. **Cache Value**:
   - Store the processed result from `processSearch(query)`.
4. **TTL (Time-to-Live)**:
   - Assign an expiration time to cached results to avoid indefinite staleness.

#### **B. Steps for Query Handling**
1. A user sends a search query to any of the 100 machines.
2. The machine checks the cache using the query as the key.
   - **Cache Hit**: Return the result immediately.
   - **Cache Miss**: Call `processSearch(query)` to compute the result, store it in the cache, and return it to the user.
3. If data changes (e.g., new documents are indexed), invalidate or update the affected cache entries.

---

### **3. Cache Consistency and Coordination**
#### **A. Centralized Cache**
- Use a single shared cache cluster (e.g., Redis or Memcached) accessible by all 100 machines.
- Pros:
  - Simplifies design.
  - Ensures all machines see the same cache state.
- Cons:
  - A single point of failure (mitigated by using replication and clustering).
  - Network latency when accessing a remote cache.

#### **B. Distributed Cache (DHT-Based)**
- Partition the cache across the 100 machines themselves using a consistent hashing mechanism.
- Pros:
  - Decentralized; avoids a single point of failure.
  - Cache lookup is localized to fewer machines.
- Cons:
  - More complex to implement.
  - Requires coordination to handle failures or rebalancing.

---

### **4. Cache Update Strategies**
When data changes, the cache needs to be updated to ensure correctness:

#### **A. Invalidation**
- Invalidate cached entries when underlying data changes.
- Steps:
  1. Detect data changes (e.g., through event notifications, database triggers, or scheduled refreshes).
  2. Identify affected cache keys and remove them.
  3. On the next query, the system will recompute the result and repopulate the cache.
- Pros:
  - Simple and ensures correctness.
  - Avoids unnecessary recomputation.
- Cons:
  - Potential for slightly increased latency during recomputation.

#### **B. Write-Through Cache**
- Update the cache whenever data changes.
- Steps:
  1. When indexing new documents or modifying data, recompute search results for relevant queries.
  2. Update the cache with the new results.
- Pros:
  - Ensures cache is always fresh.
  - No need for recomputation during query time.
- Cons:
  - Higher computational and write overhead during updates.
  - May update results for queries that are never accessed.

#### **C. Time-Based Expiration (TTL)**
- Assign a TTL to each cached entry.
- When the TTL expires, the cache entry is removed, forcing recomputation on the next query.
- Pros:
  - Simplifies invalidation logic.
  - Reduces stale data risk without explicit invalidation.
- Cons:
  - May serve stale results until TTL expires.
  - TTL needs to be carefully tuned to balance freshness and performance.

---

### **5. High-Level Workflow**

1. **Cache Layer**:
   - Queries are hashed to identify cache keys.
   - A shared cache system (e.g., Redis cluster or distributed DHT) stores results.
   
2. **Query Handling**:
   - **Step 1**: Hash the query string to get a cache key.
   - **Step 2**: Lookup in the distributed cache.
     - If found: Return the cached result.
     - If not found: Call `processSearch(query)`, store the result in the cache, and return it.

3. **Cache Updates**:
   - On data changes, use invalidation or write-through mechanisms to keep the cache consistent.
   - Optionally, use TTL to simplify update logic.

---

### **6. Failure Handling**
1. **Cache Misses**:
   - If the cache is unavailable or misses, fallback to `processSearch(query)`.
2. **Cache Server Failures**:
   - Use replication or clustering (e.g., Redis Sentinel, Memcached with replicas) to maintain availability.
3. **Consistency Issues**:
   - Use versioning or timestamps to ensure that stale data is not served.

---

### **7. Scaling the Cache**
1. **Sharding**:
   - Partition the cache based on hash ranges or consistent hashing to distribute load across multiple servers.
2. **Load Balancing**:
   - Use a load balancer to distribute requests evenly across the cache servers.
3. **Eviction Policies**:
   - Use an eviction policy (e.g., LRU) to manage memory usage in high-traffic scenarios.

---

### **8. Summary**

| **Feature**              | **Implementation**                                     |
|--------------------------|--------------------------------------------------------|
| **Shared Cache**         | Use Redis/Memcached with replication or clustering.    |
| **Distributed Cache**    | Use consistent hashing for cache partitioning.         |
| **Update Mechanism**     | Invalidate or write-through cache on data changes.     |
| **Stale Data Management**| Time-to-live (TTL) for auto-expiration of old results. |
| **Fault Tolerance**      | Replication, clustering, and load balancing.           |

This design ensures low latency, high throughput, and cache consistency while handling large-scale query loads.