# 9.4 Duplicate URLs

## Problem Description 
You have 10 billion URLs. How do you detect duplicate documents? In this case, assume "duplicate" means that the URLs are identical.

## Solution

Detecting duplicates among **10 billion URLs** where "duplicate" means identical URLs is a challenge that requires efficient data structures and distributed system design. Here's how you can design a system for this:

---

### **1. System Overview**
#### **Goal**: Detect and remove duplicate URLs among 10 billion entries.
#### **Challenges**:
1. **Scale**: Storing and processing 10 billion URLs.
2. **Performance**: Quickly identifying duplicates during insertion or batch processing.
3. **Memory Efficiency**: Avoid excessive memory usage for storing such a massive dataset.

---

### **2. System Design Strategies**

#### **A. URL Normalization**
- **Problem**: URLs can appear in multiple forms but still be identical (e.g., `http://example.com` vs. `https://example.com/`).
- **Solution**:
  - Normalize URLs by:
    - Converting to lowercase.
    - Removing redundant parameters (e.g., tracking parameters like `utm_source`).
    - Resolving relative paths.

---

#### **B. Efficient Deduplication**
To process and detect duplicates efficiently, we need to employ techniques that scale:

---

### **Option 1: Hashing with a Distributed Key-Value Store**

1. **Approach**:
   - Compute a **hash** (e.g., SHA256 or MD5) for each normalized URL.
   - Use the hash as a unique identifier.
   - Store the hash in a distributed **key-value store** (e.g., Redis, DynamoDB, Cassandra).
   - Before inserting a new URL:
     - Check if the hash already exists.
     - If yes, it's a duplicate; otherwise, insert it.

2. **Advantages**:
   - Hashes are fixed in size (e.g., 256 bits for SHA256), making them memory efficient.
   - Distributed systems allow horizontal scaling.

3. **Challenges**:
   - Hash collisions (very rare with cryptographic hashes but still possible).
   - Network overhead in distributed environments.

---

### **Option 2: Bloom Filters for Approximate Deduplication**

1. **Approach**:
   - Use a **Bloom Filter**, a probabilistic data structure that efficiently checks for set membership.
   - Insert hashes of URLs into the Bloom Filter.
   - To check if a URL is a duplicate, query the filter:
     - If the Bloom Filter says "not present," it's definitely unique.
     - If the Bloom Filter says "present," it **might** be a duplicate, so verify against a secondary database.

2. **Advantages**:
   - Extremely memory-efficient; can represent billions of URLs in just a few GB of memory.
   - Fast lookups with O(1) complexity.

3. **Challenges**:
   - False positives: Some URLs may be flagged as duplicates erroneously.
   - Requires a secondary database for verification.

---

### **Option 3: Distributed Sorting and Deduplication**

1. **Approach**:
   - Store all URLs in a distributed file system (e.g., Hadoop HDFS, AWS S3).
   - Use a **MapReduce** job to:
     - Map: Emit (normalized URL, 1) for each URL.
     - Reduce: Aggregate counts and emit only unique URLs.
   - Save the deduplicated list back to storage.

2. **Advantages**:
   - Scalable for batch processing.
   - Handles massive datasets efficiently with distributed resources.

3. **Challenges**:
   - Not suitable for real-time deduplication.
   - Requires significant compute and storage resources.

---

### **Option 4: Trie-Based Deduplication**

1. **Approach**:
   - Use a **Trie (prefix tree)** to store normalized URLs.
   - Each node represents a character, and duplicate detection occurs during insertion.
   - If a URL's path fully exists in the Trie, it's a duplicate; otherwise, insert it.

2. **Advantages**:
   - Memory efficient for URLs with common prefixes (e.g., `example.com/a`, `example.com/b`).
   - Can process URLs incrementally.

3. **Challenges**:
   - Not as efficient for distributed systems.
   - Slower lookups compared to hashing for extremely large datasets.

---

### **3. Scalability Considerations**
To handle 10 billion URLs effectively, **distributed systems** are essential:

#### **A. Sharding**
- Split URLs across multiple servers based on a hash of the URL (e.g., modulo operation on the hash).
- Each shard processes a portion of the URLs independently, reducing bottlenecks.

#### **B. Distributed Databases**
- Use distributed databases like Apache Cassandra, MongoDB, or Google Bigtable for storage and lookups.
- Ensure consistency across shards using replication and consensus protocols.

#### **C. Parallel Processing**
- Use frameworks like Apache Spark or Hadoop for batch deduplication.
- Split the URL list into chunks, process them in parallel, and merge results.

---

### **4. Workflow**
1. **Preprocessing**:
   - Normalize all URLs.
   - Generate a unique hash for each URL.

2. **Deduplication**:
   - Use a distributed hash table, Bloom Filter, or MapReduce pipeline to detect duplicates.

3. **Storage**:
   - Store deduplicated URLs in a scalable database (e.g., HDFS, DynamoDB).

4. **Verification** (if needed):
   - Use secondary checks for false positives (in Bloom Filter or hash collision cases).

---

### **5. Trade-Offs**
| Method                  | Memory Efficiency | Speed         | Scalability       | Accuracy     |
|-------------------------|-------------------|---------------|-------------------|--------------|
| Hashing + Key-Value DB  | Moderate          | Fast          | Highly Scalable   | High         |
| Bloom Filters           | High              | Very Fast     | Moderately Scalable | Approximate  |
| MapReduce Sorting       | Moderate          | Batch-Only    | Very Scalable     | High         |
| Trie-Based Deduplication| Moderate          | Moderate      | Low-Medium        | High         |

---

### **6. Final Recommendation**
- For **real-time deduplication**, use **Hashing with a Distributed Key-Value Store**.
- For **batch processing**, use **MapReduce** or **Bloom Filters** with a verification step.
- Always normalize URLs upfront to minimize duplicate entries caused by variations in URL representation.