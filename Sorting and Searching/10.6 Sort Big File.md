# 10.6 Sort Big File

## Problem Description

Imagine you have a 20gb file with one string per line. Explain how you would sort the file.

## Solution

Sorting a 20GB file with one string per line requires handling a dataset that is too large to fit into memory at once. Here's an efficient way to sort it using an **External Sorting** approach:

---

### **Steps for External Sorting**

#### 1. **Divide and Conquer (Chunking the File)**
   - **Read the file in chunks:** Since the file is too large to fit in memory, divide it into smaller chunks that can fit into the available memory.
     - Example: If your system has 1GB of memory available, read 1GB-sized chunks.
   - **Sort each chunk in memory:** For each chunk, load it into memory, sort it using an efficient in-memory sorting algorithm like quicksort or mergesort, and write the sorted chunk back to disk as a temporary file.
     - Temporary files: `chunk1.txt`, `chunk2.txt`, ..., `chunkN.txt`.

#### 2. **Merge Sorted Chunks**
   - Once all chunks are sorted and stored as temporary files, merge these sorted chunks into a single sorted file.
   - Use a **k-way merge** algorithm, where `k` is the number of chunks. This is typically implemented using a **min-heap** to efficiently find the smallest element among the current heads of all chunks.

#### 3. **Write the Final Output**
   - The merged data from the sorted chunks is written sequentially to a final output file.

---

### **Implementation Details**

#### **Chunking and Sorting**
1. Open the 20GB file.
2. Read a manageable chunk into memory (e.g., 1GB or less depending on the system).
3. Sort the chunk in memory.
4. Write the sorted chunk to a temporary file on disk.

#### **Merging Sorted Chunks**
1. Open all temporary files simultaneously.
2. Use a min-heap to keep track of the smallest element among the heads of all open files.
3. Extract the smallest element from the heap, write it to the output file, and replace it with the next element from the corresponding file.
4. Repeat until all temporary files are exhausted.

---

### **Python Example**

Here's a Python implementation outline for external sorting:

```python
import heapq
import os

def sort_large_file(input_file, chunk_size, output_file):
    # Step 1: Split into chunks and sort
    temp_files = []
    with open(input_file, 'r') as infile:
        lines = []
        for line in infile:
            lines.append(line.strip())
            if len(lines) * len(line.encode('utf-8')) >= chunk_size:  # Limit memory usage
                lines.sort()  # Sort in-memory
                temp_file = f"chunk_{len(temp_files)}.txt"
                with open(temp_file, 'w') as tmp:
                    tmp.write("\n".join(lines) + "\n")
                temp_files.append(temp_file)
                lines = []  # Clear memory

        if lines:  # Sort and save remaining lines
            lines.sort()
            temp_file = f"chunk_{len(temp_files)}.txt"
            with open(temp_file, 'w') as tmp:
                tmp.write("\n".join(lines) + "\n")
            temp_files.append(temp_file)

    # Step 2: Merge sorted chunks
    with open(output_file, 'w') as outfile:
        min_heap = []
        file_handles = [open(f, 'r') for f in temp_files]

        # Initialize heap with the first line of each file
        for i, fh in enumerate(file_handles):
            line = fh.readline().strip()
            if line:
                heapq.heappush(min_heap, (line, i))

        # Extract from heap and refill from the corresponding file
        while min_heap:
            smallest, file_index = heapq.heappop(min_heap)
            outfile.write(smallest + "\n")
            next_line = file_handles[file_index].readline().strip()
            if next_line:
                heapq.heappush(min_heap, (next_line, file_index))

        # Close all file handles and delete temporary files
        for fh in file_handles:
            fh.close()
        for temp_file in temp_files:
            os.remove(temp_file)

# Example usage
sort_large_file('large_file.txt', chunk_size=1 * 1024 * 1024 * 1024, output_file='sorted_file.txt')
```

---

### **Explanation of Parameters**
- `input_file`: The large file to be sorted.
- `chunk_size`: The size of each chunk to fit in memory.
- `output_file`: The final sorted output file.
- Temporary files are created during the process and deleted after merging.

---

### **Complexity Analysis**
1. **Time Complexity**:
   - Sorting each chunk: O($`(n/k)\cdot log(n/k)`$), where `n` is the total number of lines, and `k` is the number of chunks.
   - Merging all chunks: O($`n \cdot logk`$), where `k` is the number of chunks.
   - Overall: O($`n \cdot log(n/k) + n \cdot log(k)`$).

2. **Space Complexity**:
   - In-memory usage: O(k) for the heap during merging and O(chunk_size) for sorting each chunk.

---

### **Advantages of This Approach**
- Efficiently handles files larger than available memory.
- Minimizes memory usage by processing one chunk at a time.
- Scales well with large datasets.

